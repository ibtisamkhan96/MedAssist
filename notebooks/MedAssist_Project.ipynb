{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Install Required Packages**"
      ],
      "metadata": {
        "id": "G-O3fRzWm8_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IXJ_dAUfRvl"
      },
      "outputs": [],
      "source": [
        "print(\"Installing required packages...\")\n",
        "\n",
        "!pip install -q langchain langchain-community langchain-openai\n",
        "!pip install -q sentence-transformers faiss-cpu chromadb\n",
        "!pip install -q biopython requests beautifulsoup4 lxml\n",
        "!pip install -q nltk spacy python-dotenv\n",
        "!pip install -q groq google-generativeai openai\n",
        "!pip install -q gradio plotly pandas numpy tqdm\n",
        "!pip install -q pypdf python-magic\n",
        "\n",
        "print(\"‚úÖ All packages installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mount Google Drive and Setup Project**\n"
      ],
      "metadata": {
        "id": "RofnSvbOnIAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_path = '/content/drive/MyDrive/MedAssist_RAG'\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "os.chdir(project_path)\n",
        "\n",
        "folders = ['data/raw', 'data/processed', 'data/embeddings', 'src', 'logs', 'models']\n",
        "for folder in folders:\n",
        "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Project initialized at: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "DeqYDfl8fVj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Paste Pubmed Email and Configure API Keys**\n"
      ],
      "metadata": {
        "id": "1FNamUzXnL55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "PUBMED_EMAIL = input(\"üìß Enter email for PubMed API: \")\n",
        "os.environ['PUBMED_EMAIL'] = PUBMED_EMAIL\n",
        "\n",
        "print(\"\\nüîë Choose LLM Provider:\")\n",
        "print(\"1. Groq (Recommended - Free & Fast)\")\n",
        "print(\"2. Google Gemini\")\n",
        "\n",
        "choice = input(\"\\nEnter choice (1-2): \")\n",
        "\n",
        "if choice == \"1\":\n",
        "    GROQ_API_KEY = getpass(\"üîë Enter Groq API key: \")\n",
        "    os.environ['GROQ_API_KEY'] = GROQ_API_KEY\n",
        "    os.environ['LLM_PROVIDER'] = \"groq\"\n",
        "    print(\"‚úÖ Groq configured\")\n",
        "elif choice == \"2\":\n",
        "    GEMINI_API_KEY = getpass(\"üîë Enter Gemini API key: \")\n",
        "    os.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\n",
        "    os.environ['LLM_PROVIDER'] = \"gemini\"\n",
        "    print(\"‚úÖ Gemini configured\")\n",
        "\n",
        "print(\"\\n‚úÖ Configuration complete\")"
      ],
      "metadata": {
        "id": "WOwTbj5UfXPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Collection Configuration**\n"
      ],
      "metadata": {
        "id": "2FDFPfXjnq1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TARGET_PAPERS = 20000\n",
        "PAPERS_PER_TOPIC = 500\n",
        "\n",
        "print(f\"üìä Collection Strategy:\")\n",
        "print(f\"   Target: {TARGET_PAPERS:,} papers\")\n",
        "print(f\"   Per topic: {PAPERS_PER_TOPIC}\")\n",
        "\n",
        "os.environ['TARGET_PAPERS'] = str(TARGET_PAPERS)\n",
        "os.environ['PAPERS_PER_TOPIC'] = str(PAPERS_PER_TOPIC)"
      ],
      "metadata": {
        "id": "EBMdGCRrfZ9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define Medical Domains**\n"
      ],
      "metadata": {
        "id": "V8jmwR3fn09b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MEDICAL_DOMAINS = {\n",
        "    \"cardiovascular\": [\n",
        "        \"myocardial infarction\", \"heart failure\", \"atrial fibrillation\",\n",
        "        \"coronary artery disease\", \"hypertension\", \"stroke\", \"angina\"\n",
        "    ],\n",
        "    \"endocrine\": [\n",
        "        \"diabetes mellitus type 1\", \"diabetes mellitus type 2\",\n",
        "        \"thyroid disorders\", \"metabolic syndrome\", \"obesity\"\n",
        "    ],\n",
        "    \"neurological\": [\n",
        "        \"alzheimer disease\", \"parkinson disease\", \"multiple sclerosis\",\n",
        "        \"epilepsy\", \"migraine\", \"dementia\"\n",
        "    ],\n",
        "    \"respiratory\": [\n",
        "        \"asthma\", \"chronic obstructive pulmonary disease\", \"pneumonia\",\n",
        "        \"tuberculosis\", \"lung cancer\", \"COVID-19\"\n",
        "    ],\n",
        "    \"gastrointestinal\": [\n",
        "        \"inflammatory bowel disease\", \"crohn disease\", \"ulcerative colitis\",\n",
        "        \"hepatitis\", \"cirrhosis\", \"pancreatitis\"\n",
        "    ],\n",
        "    \"oncology\": [\n",
        "        \"lung cancer\", \"breast cancer\", \"colorectal cancer\",\n",
        "        \"chemotherapy\", \"radiation therapy\", \"immunotherapy\"\n",
        "    ],\n",
        "    \"psychiatric\": [\n",
        "        \"major depressive disorder\", \"anxiety disorders\", \"schizophrenia\",\n",
        "        \"bipolar disorder\", \"ADHD\", \"autism spectrum disorder\"\n",
        "    ],\n",
        "    \"infectious\": [\n",
        "        \"HIV AIDS\", \"COVID-19\", \"influenza\", \"tuberculosis\",\n",
        "        \"hepatitis B\", \"hepatitis C\", \"sepsis\"\n",
        "    ],\n",
        "    \"medications\": [\n",
        "        \"antibiotics\", \"antihypertensives\", \"antidiabetic agents\",\n",
        "        \"statins\", \"anticoagulants\", \"beta blockers\", \"ACE inhibitors\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "total_topics = sum(len(topics) for topics in MEDICAL_DOMAINS.values())\n",
        "print(f\"üìö Medical domains configured: {len(MEDICAL_DOMAINS)} categories, {total_topics} topics\")"
      ],
      "metadata": {
        "id": "zrdbX1spfbKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Collection from PubMed**\n"
      ],
      "metadata": {
        "id": "WUMUrdFZn7_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez, Medline\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "Entrez.email = os.getenv('PUBMED_EMAIL')\n",
        "\n",
        "def search_pubmed(query, max_results=2000):\n",
        "    search_term = f\"({query}) AND 2010:2024[PDAT] AND English[LA]\"\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_results, sort=\"relevance\")\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "        return record[\"IdList\"]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def fetch_papers(pmid_list):\n",
        "    papers = []\n",
        "    for i in tqdm(range(0, len(pmid_list), 100), desc=\"Fetching\", leave=False):\n",
        "        batch = pmid_list[i:i+100]\n",
        "        try:\n",
        "            handle = Entrez.efetch(db=\"pubmed\", id=batch, rettype=\"medline\", retmode=\"text\")\n",
        "            records = Medline.parse(handle)\n",
        "            for record in records:\n",
        "                abstract = record.get(\"AB\", \"\")\n",
        "                if abstract and len(abstract) > 200:\n",
        "                    papers.append({\n",
        "                        \"pmid\": record.get(\"PMID\", \"\"),\n",
        "                        \"title\": record.get(\"TI\", \"\"),\n",
        "                        \"abstract\": abstract,\n",
        "                        \"authors\": \", \".join(record.get(\"AU\", [])),\n",
        "                        \"journal\": record.get(\"TA\", \"\"),\n",
        "                        \"publication_date\": record.get(\"DP\", \"\"),\n",
        "                        \"url\": f\"https://pubmed.ncbi.nlm.nih.gov/{record.get('PMID', '')}/\"\n",
        "                    })\n",
        "            handle.close()\n",
        "            time.sleep(0.3)\n",
        "        except:\n",
        "            continue\n",
        "    return papers\n",
        "\n",
        "print(\"üîç Starting data collection...\")\n",
        "\n",
        "all_papers = []\n",
        "target = int(os.getenv('TARGET_PAPERS', '20000'))\n",
        "per_topic = int(os.getenv('PAPERS_PER_TOPIC', '500'))\n",
        "\n",
        "for category, topics in MEDICAL_DOMAINS.items():\n",
        "    if len(all_papers) >= target:\n",
        "        break\n",
        "    print(f\"\\nüìñ {category.upper()}\")\n",
        "    for topic in tqdm(topics):\n",
        "        pmids = search_pubmed(topic, per_topic)\n",
        "        papers = fetch_papers(pmids)\n",
        "        for p in papers:\n",
        "            p['category'] = category\n",
        "            p['topic'] = topic\n",
        "        all_papers.extend(papers)\n",
        "        time.sleep(0.5)\n",
        "    print(f\"‚úÖ {len([p for p in all_papers if p['category']==category])} papers\")\n",
        "\n",
        "seen = set()\n",
        "unique_papers = []\n",
        "for paper in tqdm(all_papers, desc=\"Deduplicating\"):\n",
        "    pmid = paper.get(\"pmid\")\n",
        "    if pmid and pmid not in seen:\n",
        "        seen.add(pmid)\n",
        "        unique_papers.append(paper)\n",
        "\n",
        "output_path = 'data/raw/medical_papers.json'\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(unique_papers, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "df = pd.DataFrame(unique_papers)\n",
        "df.to_csv('data/raw/medical_papers.csv', index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Collected {len(unique_papers):,} unique papers\")\n",
        "print(f\"üíæ Saved to {output_path}\")"
      ],
      "metadata": {
        "id": "F-2Fuzr7fb6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Preprocessing and Chunking**\n"
      ],
      "metadata": {
        "id": "ssRRXPgAoA1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 100\n",
        "\n",
        "with open('/content/drive/MyDrive/MedAssist_RAG/data/raw/pubmed_papers.json', 'r') as f:\n",
        "    papers = json.load(f)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\-\\( \\[\\].;,:/%]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def estimate_tokens(text):\n",
        "    return len(text) // 4\n",
        "\n",
        "def chunk_text(text, chunk_size=800, overlap=100):\n",
        "    text = clean_text(text)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    chunks = []\n",
        "    current = []\n",
        "    length = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_len = estimate_tokens(sent)\n",
        "        if length + sent_len > chunk_size and current:\n",
        "            chunks.append(' '.join(current))\n",
        "            current = current[-2:] if len(current) >= 2 else []\n",
        "            length = estimate_tokens(' '.join(current))\n",
        "        current.append(sent)\n",
        "        length += sent_len\n",
        "\n",
        "    if current:\n",
        "        chunks.append(' '.join(current))\n",
        "    return chunks\n",
        "\n",
        "all_chunks = []\n",
        "chunk_id = 0\n",
        "\n",
        "for paper in tqdm(papers, desc=\"Chunking\"):\n",
        "    title = paper.get('title', '')\n",
        "    abstract = paper.get('abstract', '')\n",
        "    full_text = f\"Title: {title}\\n\\nAbstract: {abstract}\"\n",
        "\n",
        "    text_chunks = chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        all_chunks.append({\n",
        "            'chunk_id': f\"chunk_{chunk_id}\",\n",
        "            'text': chunk,\n",
        "            'metadata': {\n",
        "                'title': title,\n",
        "                'pmid': paper.get('pmid', ''),\n",
        "                'category': paper.get('category', ''),\n",
        "                'topic': paper.get('topic', ''),\n",
        "                'url': paper.get('url', ''),\n",
        "                'chunk_index': i\n",
        "            }\n",
        "        })\n",
        "        chunk_id += 1\n",
        "\n",
        "output_path = 'data/processed/chunks.json'\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Created {len(all_chunks):,} chunks from {len(papers):,} papers\")\n",
        "print(f\"üíæ Saved to {output_path}\")"
      ],
      "metadata": {
        "id": "eKk2A7hWffrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Generate Embeddings**\n"
      ],
      "metadata": {
        "id": "VGKJuoQyoGtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
        "\n",
        "with open('/content/drive/MyDrive/MedAssist_RAG/data/processed/chunks.json', 'r') as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "texts = [chunk['text'] for chunk in chunks]\n",
        "\n",
        "print(f\"Generating embeddings for {len(texts):,} chunks...\")\n",
        "embeddings = model.encode(\n",
        "    texts,\n",
        "    batch_size=32,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True\n",
        ")\n",
        "\n",
        "output_dir = Path('data/embeddings')\n",
        "np.save(output_dir / 'embeddings.npy', embeddings)\n",
        "\n",
        "with open(output_dir / 'chunks_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(chunks, f)\n",
        "\n",
        "print(f\"‚úÖ Embeddings generated: {embeddings.shape}\")\n",
        "print(f\"üíæ Saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "z59m29mBfhbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build FAISS Vector Store**\n"
      ],
      "metadata": {
        "id": "KTzCRlNFoNQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "embeddings = np.load('/content/drive/MyDrive/MedAssist_RAG/data/embeddings/embeddings.npy')\n",
        "with open('/content/drive/MyDrive/MedAssist_RAG/data/embeddings/chunks_metadata.pkl', 'rb') as f:\n",
        "    chunks = pickle.load(f)\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "num_vectors = embeddings.shape[0]\n",
        "\n",
        "if num_vectors < 10000:\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    index_type = \"Flat\"\n",
        "elif num_vectors < 100000:\n",
        "    nlist = min(100, num_vectors // 100)\n",
        "    quantizer = faiss.IndexFlatIP(dimension)\n",
        "    index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
        "    index.train(embeddings.astype('float32'))\n",
        "    index.nprobe = min(10, nlist)\n",
        "    index_type = \"IVF\"\n",
        "else:\n",
        "    nlist = 200\n",
        "    quantizer = faiss.IndexFlatIP(dimension)\n",
        "    index = faiss.IndexIVFPQ(quantizer, dimension, nlist, 8, 8)\n",
        "    index.train(embeddings.astype('float32'))\n",
        "    index.nprobe = 10\n",
        "    index_type = \"IVFPQ\"\n",
        "\n",
        "index.add(embeddings.astype('float32'))\n",
        "\n",
        "faiss.write_index(index, 'data/embeddings/faiss_index.bin')\n",
        "\n",
        "retriever_code = '''\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "\n",
        "class MedicalRetriever:\n",
        "    def __init__(self):\n",
        "        self.index = faiss.read_index(\"data/embeddings/faiss_index.bin\")\n",
        "        with open(\"data/embeddings/chunks_metadata.pkl\", \"rb\") as f:\n",
        "            self.chunks = pickle.load(f)\n",
        "        self.embeddings = np.load(\"data/embeddings/embeddings.npy\")\n",
        "\n",
        "    def search(self, query_embedding, top_k=5):\n",
        "        if len(query_embedding.shape) == 1:\n",
        "            query_embedding = query_embedding.reshape(1, -1)\n",
        "        distances, indices = self.index.search(query_embedding.astype(\"float32\"), top_k)\n",
        "        results = []\n",
        "        for idx, score in zip(indices[0], distances[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                results.append({\n",
        "                    \"score\": float(score),\n",
        "                    \"text\": self.chunks[idx][\"text\"],\n",
        "                    \"metadata\": self.chunks[idx][\"metadata\"]\n",
        "                })\n",
        "        return results\n",
        "'''\n",
        "\n",
        "with open('src/retriever.py', 'w') as f:\n",
        "    f.write(retriever_code)\n",
        "\n",
        "print(f\"‚úÖ FAISS index built: {index_type}, {index.ntotal:,} vectors\")"
      ],
      "metadata": {
        "id": "HTpWnAzdfjGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Create RAG Pipeline**\n"
      ],
      "metadata": {
        "id": "E1ACy31NoRs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure the absolute path to 'src' is in sys.path\n",
        "src_path = os.path.abspath('src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# If 'retriever' module was loaded previously in a bad state, remove it from sys.modules\n",
        "if 'retriever' in sys.modules:\n",
        "    del sys.modules['retriever']\n",
        "\n",
        "from retriever import MedicalRetriever\n",
        "\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
        "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
        "TEMPERATURE = 0.1\n",
        "MAX_TOKENS = 2000\n",
        "\n",
        "SYSTEM_PROMPT = '''You are MedAssist, an expert medical research assistant.\n",
        "\n",
        "Provide comprehensive, evidence-based answers with:\n",
        "1. Clear medical terminology with explanations\n",
        "2. Citations to sources as [Source N]\n",
        "3. Detailed mechanisms and treatments\n",
        "4. Professional medical language\n",
        "\n",
        "DISCLAIMER: For educational purposes only. Consult healthcare professionals for medical advice.'''\n",
        "\n",
        "embedding_model = SentenceTransformer(MODEL_NAME)\n",
        "retriever = MedicalRetriever()\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "class MedicalRAG:\n",
        "    def __init__(self, retriever, embedding_model, groq_client, llm_model):\n",
        "        self.retriever = retriever\n",
        "        self.embedding_model = embedding_model\n",
        "        self.groq_client = groq_client\n",
        "        self.llm_model = llm_model\n",
        "\n",
        "    def query(self, question, top_k=10):\n",
        "        query_embedding = self.embedding_model.encode([question], normalize_embeddings=True)\n",
        "        results = self.retriever.search(query_embedding, top_k=top_k)\n",
        "\n",
        "        context_parts = []\n",
        "        for i, result in enumerate(results, 1):\n",
        "            meta = result['metadata']\n",
        "            context_parts.append(f\"[Source {i}: {meta['title']}]\\n{result['text']}\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        prompt = f\"\"\"Based on medical research literature, provide a comprehensive answer.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "Provide a detailed answer with source citations [Source N].\"\"\"\n",
        "\n",
        "        response = self.groq_client.chat.completions.create(\n",
        "            model=self.llm_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=TEMPERATURE,\n",
        "            max_tokens=MAX_TOKENS\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"answer\": response.choices[0].message.content,\n",
        "            \"sources\": [{\"source_id\": i, \"title\": r['metadata']['title'], \"url\": r['metadata']['url']}\n",
        "                       for i, r in enumerate(results, 1)]\n",
        "        }\n",
        "\n",
        "rag_system = MedicalRAG(retriever, embedding_model, groq_client, LLM_MODEL)\n",
        "\n",
        "def ask_medical_question(question, top_k=10):\n",
        "    return rag_system.query(question, top_k=top_k)\n",
        "\n",
        "print(\"‚úÖ RAG pipeline ready\")"
      ],
      "metadata": {
        "id": "DSuncF19flBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Launch Gradio Interface**\n"
      ],
      "metadata": {
        "id": "AyitSUCaoXI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(question, num_sources):\n",
        "    if not question.strip():\n",
        "        return \"‚ö†Ô∏è Please enter a question\", \"\", \"\"\n",
        "\n",
        "    try:\n",
        "        response = ask_medical_question(question, top_k=int(num_sources))\n",
        "\n",
        "        answer = f\"## üí° Answer\\n\\n{response['answer']}\\n\\n---\\n\\n**Sources:** {len(response['sources'])} papers\"\n",
        "\n",
        "        sources = \"## üìö Sources\\n\\n\"\n",
        "        for source in response['sources'][:int(num_sources)]:\n",
        "            sources += f\"**[{source['source_id']}]** {source['title']}\\n[View Paper]({source['url']})\\n\\n\"\n",
        "\n",
        "        disclaimer = \"\"\"## ‚ö†Ô∏è Disclaimer\n",
        "\n",
        "**Educational purposes only.** Not a substitute for professional medical advice.\n",
        "\n",
        "Always consult qualified healthcare professionals for medical decisions.\"\"\"\n",
        "\n",
        "        return answer, sources, disclaimer\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\", \"\", \"\"\n",
        "\n",
        "sample_questions = {\n",
        "    \"ü´Ä Cardiovascular\": [\"What are treatments for Type 2 diabetes?\", \"Explain heart failure pathophysiology\"],\n",
        "    \"üíä Pharmacology\": [\"What are statin side effects?\", \"Explain metformin mechanism\"],\n",
        "    \"üß¨ Oncology\": [\"How does chemotherapy work?\", \"What is immunotherapy?\"],\n",
        "    \"üß† Neurology\": [\"What causes Alzheimer's disease?\", \"Explain Parkinson's disease\"],\n",
        "}\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"indigo\"), title=\"MedAssist\") as demo:\n",
        "    gr.HTML(\"\"\"\n",
        "        <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 15px; color: white; text-align: center;\">\n",
        "            <h1 style=\"font-size: 3em; margin: 0;\">üè• MedAssist</h1>\n",
        "            <p style=\"font-size: 1.3em; margin-top: 10px;\">AI-Powered Medical Research Assistant</p>\n",
        "            <p>20,000+ Papers ‚Ä¢ Powered by Groq AI</p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## üìù Ask Your Question\")\n",
        "            question = gr.Textbox(label=\"\", placeholder=\"Type medical question...\", lines=4)\n",
        "            num_sources = gr.Slider(3, 10, 5, step=1, label=\"üìä Number of Sources\")\n",
        "            submit_btn = gr.Button(\"üîç Get Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            gr.Markdown(\"### üí° Sample Questions\")\n",
        "            for category, questions in sample_questions.items():\n",
        "                with gr.Accordion(category, open=False):\n",
        "                    for q in questions:\n",
        "                        gr.Button(q, size=\"sm\").click(fn=lambda x=q: x, outputs=question)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.HTML(\"\"\"\n",
        "                <div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 20px; border-radius: 10px; color: white;\">\n",
        "                    <h3>üìä Statistics</h3>\n",
        "                    <p><strong>Papers:</strong> 20,000+</p>\n",
        "                    <p><strong>Chunks:</strong> 48,955</p>\n",
        "                    <p><strong>Model:</strong> Llama 3.3 70B</p>\n",
        "                    <p><strong>Speed:</strong> 800 tokens/sec</p>\n",
        "                </div>\n",
        "            \"\"\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"üí° Answer\"):\n",
        "            answer_output = gr.Markdown()\n",
        "        with gr.Tab(\"üìö Sources\"):\n",
        "            sources_output = gr.Markdown()\n",
        "        with gr.Tab(\"‚ö†Ô∏è Disclaimer\"):\n",
        "            disclaimer_output = gr.Markdown()\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=answer_question,\n",
        "        inputs=[question, num_sources],\n",
        "        outputs=[answer_output, sources_output, disclaimer_output]\n",
        "    )\n",
        "\n",
        "    gr.Examples(\n",
        "        examples=[[\"What are treatments for Type 2 diabetes?\", 5], [\"Explain heart failure\", 5]],\n",
        "        inputs=[question, num_sources]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "Ywmy9IvNfnhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}